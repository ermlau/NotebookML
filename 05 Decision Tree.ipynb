{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’Albero Decisionale o Decision Tree è un algoritmo di tipo supervisionato usato in Statistica, Data Mining e Machine Learning.\n",
    "\n",
    "E’ un modello predittivo che può essere usato sia per casi di classificazione, sia per casi di regressione (clicca qui per ripassare i concetti). Questo vuol dire che l’output che andrà a prevedere il modello sarà o una variabile categorica (per esempio una binaria SI/NO) o una quantità continua, come per esempio il prezzo di una casa.\n",
    "\n",
    "Gli alberi decisionali sono gli algoritmi più comuni perché sono molto veloci e soprattutto semplici da interpretare. Questo è di fondamentale importanza in alcuni settori specifici in cui si può preferire una maggior semplicità di comprensione ad una maggior accuratezza del modello."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COME FUNZIONA UN ALBERO DECISIONALE\n",
    "\n",
    "In questo algoritmo i dati in input vengono continuamente splittati in base a certi criteri. Due concetti chiave per capire il loro funzionamento sono nodi e foglie. I nodi sono i luoghi in cui, in base a certe regole, i dati vengono splittati. Le foglie sono invece i risultati intermedi o finali, i luoghi in cui finiscono i dati una volta separati.\n",
    "\n",
    "Ma in base a quali criteri i dati vengono separati?\n",
    "\n",
    "L’obiettivo è quello di trovare i valori della variabile per cui otteniamo il migliore split. La scelta del migliore può esser fatta attraverso varie metriche, a seconda se sto affrontando una caso si classificazione o regressione. Per esempio per i casi di classificazione posso usare l’entropia o l’indice di Gini.\n",
    "\n",
    "In generale comunque l’obiettivo è quello di dividere la popolazione iniziale per il valore di una variabile che permette di creare due gruppi il più omogenei possibile internamente e il più disomogenei possibile tra loro.\n",
    "\n",
    "Un esempio semplice di Decision Tree è : un nodo che verifica la condizione \"la casa possiede più di due bagni?\" Se è sì avremo un certo prezzo, se è no ne avremo un altro. In questo caso la variabile numero di bagni è quella che garantiva il miglior split per determinare il prezzo della casa.\n",
    "\n",
    "Un'altro esempio più complesso, potrebbe essere considerare le altre variabili in gioco per determinare il prezzo della casa. \n",
    "\n",
    "La logica è sempre la stessa: si cerca il nodo (la condizione) che permette di generare le foglie in cui si distribuiscono meglio le osservazioni del nostro dataset.\n",
    "\n",
    "Così facendo sarà facile inserire i dati di una nuova casa e ricavarne una predizione sul suo prezzo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRINCIPALI IPERPARAMETRI DEL MODELLO\n",
    "Vediamo quali sono alcuini dei principali parametri o iperparametri di un modello Decision Tree, prendendo in considerazione l’implementazione della libreria scikit-learn in Python.\n",
    "\n",
    "Tra i parametri da tenere fortemente in considerazione c’è la metrica di splitting, che, misurando la qualità dello split, separerà i dati. Questo parametro si chiama **criterion** e dipende dal task che devo svolgere.\n",
    "\n",
    "Per quanto riguarda la classificazione, può avere “gini” e “entropy” come possibili valori. **“Gini”** fa riferimento all’impurità di Gini, mentre **“entropy”** all’information gain, basato sul concetto di entropia nella teoria dell’informazione. Per la regressione invece **“mse”**, **“friedman_mse”**, **“mae”**, e **“poisson”**.Il valore di default è l’mse, ovvero il mean squared error (l’errore quadratico medio).\n",
    "\n",
    "Un altro parametro da controllare è quello che si chiama **max_depth**: questo mi dice quanto profondo sarà l’albero. Più sarà profondo, più split farà e più sarà preciso sui dati di addestramento. Questo però non vuol dire che se è più preciso sui dati di train il modello sarà in grado di generalizzare bene, anzi! Più sarà profondo, più preciso sarà sui dati di train, meno probabilmente sarà in grado di generalizzare su dati nuovi.\n",
    "\n",
    "Una maggior profondità quindi rischia di far cadere in quel fenomeno chiamato overfitting, ovvero un’ottima capacità sul train ma una pessima sul test, che si traduce in incapacità di generalizzare su nuovi dati (che è quello che ci interessa).\n",
    "\n",
    "Un altro dei parametri che può portare overfitting è **min_samples_leaf**, ovvero quanti esemplari minimi di osservazioni possono finire nelle foglie finali. Un numero basso vorrà dire anche in questo caso una maggior precisione sul train, che potrà portare, di nuovo, ad una scarsa accuratezza sul test. Questo perché si andranno a identificare i casi più specifici e non quelli generali.\n",
    "\n",
    "Altro iperparametro importante è **max_features**: questo parametro ci dà la possibilità di scegliere in numero di variabili da utilizzare per trovare il best split. E’ inizializzato a None, quindi verrà cercato su tutte le variabili. Posso però decidere per esempio di cercarlo sulla metà delle variabili o sulla radice quadrata del numero delle variabili (se ne ho 16, lo cerco su 4).\n",
    "\n",
    "Ci siamo concentrati sui parametri che possono comportare l’overfitting perché è in base a questo concetto che il modello potrà raggiungere un risultato soddisfacente o meno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRO E CONTRO DELL’ALBERO DECISIONALE\n",
    "Avendo visto più o meno come funziona il modello e quali sono i suoi principali variabili, possiamo tirare le somme su questo algoritmo tracciandone pro e contro.\n",
    "\n",
    "### PRO\n",
    " - Molto semplice da capire e interpretare\n",
    " - Può lavorare su dati numerici e categorici contemporaneamente\n",
    " - Non richiede moltissimi dati\n",
    " - Performa bene sui su pochi dati sia su grandi dataset\n",
    "\n",
    "### CONTRO\n",
    " - Non è un modello robusto: un piccolo cambiamento nel training può comportare un grandissimo cambiamento nella prediction\n",
    " - Altissimo rischio di overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codice di esempio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.93333333, 1.        , 0.93333333, 0.93333333,\n",
       "       0.86666667, 0.93333333, 1.        , 1.        , 1.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# carico roba\n",
    "from sklearn.datasets import load_iris # referenzio il dataset dell'iris\n",
    "from sklearn.model_selection import cross_val_score  # \n",
    "#from sklearn.tree import DecisionTreeClassifier  # carico le api per il decision tree\n",
    "from sklearn import tree\n",
    "\n",
    "iris = load_iris()  # carico dati\n",
    "clf = tree.DecisionTreeClassifier(random_state=0 )  # creo l'oggetto per il decision tree\n",
    "model = clf.fit(iris.data, iris.target)\n",
    "\n",
    "scores = cross_val_score(model, iris.data, iris.target, cv=10)  # usa la k-fold validation (K=cv=10) per fare il vedere la bontà del modello\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature_3 <= 0.80\n",
      "|   |--- class: 0\n",
      "|--- feature_3 >  0.80\n",
      "|   |--- feature_3 <= 1.75\n",
      "|   |   |--- feature_2 <= 4.95\n",
      "|   |   |   |--- feature_3 <= 1.65\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature_3 >  1.65\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |--- feature_2 >  4.95\n",
      "|   |   |   |--- feature_3 <= 1.55\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |   |--- feature_3 >  1.55\n",
      "|   |   |   |   |--- feature_2 <= 5.45\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- feature_2 >  5.45\n",
      "|   |   |   |   |   |--- class: 2\n",
      "|   |--- feature_3 >  1.75\n",
      "|   |   |--- feature_2 <= 4.85\n",
      "|   |   |   |--- feature_1 <= 3.10\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |   |--- feature_1 >  3.10\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- feature_2 >  4.85\n",
      "|   |   |   |--- class: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_representation = tree.export_text(model)\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# trovare il modo per visualizzare l'albero \n",
    "# vedere https://mljar.com/blog/visualize-decision-tree/\n",
    "# e https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html\n",
    "\n",
    "# lanciando la secondo istruzione si resetta il kernel!!!!\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "_ = tree.plot_tree(model, \n",
    "                   feature_names=iris.feature_names,  \n",
    "                   class_names=iris.target_names,\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
